{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports here\n",
    "import duo_etl\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 13:38:31 WARN Utils: Your hostname, MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
      "22/04/11 13:38:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/11 13:38:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "spark = duo_etl.create_spark_session(mode='local')\n",
    "#path = '/home/e/Projects/duo-pipeline/data_files/'\n",
    "path = '../data_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read learning traces into df\n",
    "filename = 'learning_traces.csv'\n",
    "lt_df = duo_etl.read_learning_traces(spark, path, filename)\n",
    "\n",
    "# read language reference table into df\n",
    "filename = 'language-codes-full_json.json'\n",
    "lang_df = duo_etl.read_lang_ref(spark, path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file containing breakdown of lexeme codes\n",
    "filename = 'lexeme_reference.txt'\n",
    "lex_df = duo_etl.read_lex_ref_local(spark, path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning traces dataset has 12854226 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "duo_etl.show_size('learning traces', lt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexeme reference dataset has 22 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "duo_etl.show_size('lexeme reference', lex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language reference dataset has 487 rows\n"
     ]
    }
   ],
   "source": [
    "duo_etl.show_size('language reference', lang_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p_recall',\n",
       " 'timestamp',\n",
       " 'delta',\n",
       " 'user_id',\n",
       " 'learning_language',\n",
       " 'ui_language',\n",
       " 'lexeme_id',\n",
       " 'lexeme_string',\n",
       " 'history_seen',\n",
       " 'history_correct',\n",
       " 'session_seen',\n",
       " 'session_correct']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "An integral part of of data pipelines is performing data quality checks. In this case we are concerned with missing data, and duplicate data.\n",
    "\n",
    "#### Cleaning Steps\n",
    "All of the raw data is checked for missing values, and the entire rows are dropped if found.\n",
    "\n",
    "Duplicate data also presents a problem, so all duplicate rows are dropped from the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean learning traces\n",
    "lt_df = duo_etl.check_data(lt_df, 'learning traces data')\n",
    "\n",
    "# clean language reference\n",
    "lang_df = duo_etl.check_data(lang_df, 'language data', cols=['alpha2', 'English'])\n",
    "\n",
    "# clean lexeme data\n",
    "lex_df = duo_etl.check_data(lex_df, 'lexeme data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "![alt text](schema_diagram.png \"schema\")\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension table: users\n",
    "dim_users = duo_etl.create_users_table(lt_df)\n",
    "\n",
    "# dimension table: times\n",
    "dim_times = duo_etl.create_times_table(lt_df)\n",
    "\n",
    "# dimension table: languages\n",
    "dim_langs = duo_etl.create_langs_table(spark, lt_df, lang_df)\n",
    "\n",
    "# dimension table: words\n",
    "dim_words = duo_etl.create_words_table(lt_df, lex_df)\n",
    "\n",
    "# fact table: word views\n",
    "fact_wordviews = duo_etl.create_wordviews_table(lt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pkey\n",
    "pk = duo_etl.qc_check_pk_unique(dim_users, 'user_id')\n",
    "if pk != True:\n",
    "    print('qc failed for users table')\n",
    "\n",
    "# check pkey\n",
    "pk = duo_etl.qc_check_pk_unique(dim_langs, 'alpha2_code')\n",
    "if pk != True:\n",
    "    print('qc failed for langs table')\n",
    "\n",
    "# check pkey words table\n",
    "pk = duo_etl.qc_check_pk_unique(dim_words, 'lexeme_id')\n",
    "if pk != True:\n",
    "    print('qc failed for words table')\n",
    "\n",
    "# check pkey times table\n",
    "pk = duo_etl.qc_check_pk_unique(dim_times, 'epoch')\n",
    "if pk != True:\n",
    "    print('qc failed for times table')\n",
    "\n",
    "# count number of rows in learning traces... compare with size of word views table\n",
    "count = duo_etl.qc_source_count(lt_df, fact_wordviews)\n",
    "if count != True:\n",
    "    print('qc failed for wordviews table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in S3 bucket to store parquet files\n",
    "output_data = 'output_files/'\n",
    "\n",
    "# write parquet files to S3\n",
    "duo_etl.upload_parquet(s3_path, output_data, dim_times, 'dim_times.parquet')\n",
    "duo_etl.upload_parquet(s3_path, output_data, dim_langs, 'dim_langs.parquet')\n",
    "duo_etl.upload_parquet(s3_path, output_data, dim_users, 'dim_users.parquet')\n",
    "duo_etl.upload_parquet(s3_path, output_data, dim_words, 'dim_words.parquet')\n",
    "duo_etl.upload_parquet(s3_path, output_data, fact_wordviews, 'fact_wordviews.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dim_langs\n",
    "##### *alpha2_code*: the two-letter alphanumeric code used by ISO for identifying languages  (from learning traces table)\n",
    "##### *english_name*: the name of the language in English (from language reference table)\n",
    "\n",
    "### dim_times\n",
    "##### *timestamp*: timestamp of session (derived from epoch)\n",
    "##### *epoch*: Unix epoch of session (from learning traces table)\n",
    "##### *hour*: hour of session (derived from epoch)\n",
    "##### *day*: day of session (derived from epoch)\n",
    "##### *week*: week of session (derived from epoch)\n",
    "##### *month*: month of session (derived from epoch)\n",
    "##### *year*: year of session (derived from epoch)\n",
    "##### *weekday*: day of week of session (derived from epoch)\n",
    "\n",
    "### dim_users\n",
    "##### *user_id*: user ID (from learning traces table)\n",
    "##### *number_of_sessions* number of sessions user has logged (derived from learning traces table)\n",
    "\n",
    "### dim_words:\n",
    "##### *lexeme_id*: lexeme ID (from learning traces table)\n",
    "##### *language*: language of the word (from learning traces table)\n",
    "##### *lemma*: lemma of word (derived from learning traces table)\n",
    "##### *surface*: surface of word (derived from learning traces table)\n",
    "##### *part_of_speech*: part of speech of word (derived from learning traces and lexeme reference)\n",
    "\n",
    "### fact_wordviews:\n",
    "##### *timestamp*: timestamp of session\n",
    "##### *user_id*: user ID\n",
    "##### *delta*: time since word last seen\n",
    "##### *learning_language*: language that user is learning\n",
    "##### *ui_language*: language that user is using\n",
    "##### *lexeme_id*: word ID\n",
    "##### *session_pct*: percent that user has gotten the word correct in current session\n",
    "##### *history_pct*: percent that user has gotten the word correct in all previous sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view language pairs available\n",
    "lang_pairs = duo_etl.languages_available(fact_wordviews, dim_langs)\n",
    "lang_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysts to see number of users per language pair\n",
    "pair_users = duo_etl.num_users_pair(fact_wordviews)\n",
    "pair_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysts to see number of words shown per language pair\n",
    "pair_views = duo_etl.num_views_pair(fact_wordviews)\n",
    "pair_views.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses S3 to store the datasets because S3 is a cost effective way to store data in the cloud. Spark is used because it a good tool for wrangling data due to its Python API, and it can scale horizontally so that as the dataset becomes larger, it is able to handle the load.\n",
    "\n",
    "A star schema was used to model the data because each field in the word_views fact table can be further described in a corresponding dimension table. In this use-case using a star schema has the benefit over other schemas because it allows the fact table to be as minimally descriptive as it can be, with more granular information just a JOIN away in a dimension table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be updated any time there is a new learning_traces.csv and an accompanying lexeme_reference.txt dataset released. This is because the learning_traces.csv dataset is what contains the events, and the lexeme_reference.txt pairs with the words in the events. We do not expect the language_reference-json.json dataset to be updated as it is a fixed reference table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data was increased by 100x, a larger Spark cluster would have to be invoked. Likely a managed cluster such as an AWS EMR, or a Databricks cluster. \n",
    "\n",
    "If the data is used to populate a dashboard that must be updated on a daily basis, then Airflow would be used to schedule the loading of the updated learning_traces.csv file, then perform the data modeling, then upload the modeled data to S3 so that the dashboard can fetch the newly updated data.\n",
    "\n",
    "If the database needed to be accessed by 100+ people then a Redshift cluster would be made available for the people who need privilege to the data. This would ensure ACID compliance amongst all users of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
