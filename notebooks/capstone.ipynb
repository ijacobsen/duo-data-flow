{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duolingo Word Views Data Pipeline\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Duolingo is a company who's product is a language learning app. The app uses statistical techniques to optimize their user's speed and efficacy of learning languages.\n",
    "\n",
    "In this project a data piepline is created for the use of the Duolingo researchers to help better understand their users behavior within the app.\n",
    "\n",
    "Some questions that Duolingo researchers may ask are:\n",
    "\n",
    "*What are the most common language pairs?*\n",
    "\n",
    "*Which language pair has the most activity?*\n",
    "\n",
    "*Are certain language pairs correlated with time-of-day?*\n",
    "\n",
    "*Which language pair has the best retention?*\n",
    "\n",
    "*Which language UI has the highest word retention across all learning languages?*\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports here\n",
    "import duo_etl\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Duolingo has made a data set available for public use, \"learning_traces.csv\", containing instances word views in the language they are learning. Each row of data contains the users ID, the language they are learning, the word, and various other statistics relevant to the users current session and history.\n",
    "\n",
    "Duolingo has also made an accompanying data file available, \"lexeme_reference.txt\", which breaksdown lingustical attributes of the words used in \"learning_traces.csv\".\n",
    "\n",
    "The final data file, \"language-codes-full_json.json\", contains the list ISO language codes, which are present in the \"learning_traces.csv\" data set.\n",
    "\n",
    "This data pipeline downloads each of the three datasets mentioned above from S3, loads the data into separate Spark dataframes, cleans the data, reorganizes the data into a data model suited to aid in the analysis, writes the data to parquet files on S3 that can easily be loaded into Redshift for the analysts to run queries on.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "*learning_traces.csv* can be downloaded from Duolingo at https://github.com/duolingo/halflife-regression\n",
    "\n",
    "*lexeme_reference.txt* can be downloaded from Duolingo at https://github.com/duolingo/halflife-regression\n",
    "\n",
    "*language-codes-full_json.json* can be downloaded from https://datahub.io/core/language-codes#resource-language-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load AWS credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# s3 bucket where datasets live\n",
    "s3_path = 's3a://duo-data-pipeline/'\n",
    "s3_bucket = 'duo-data-pipeline'\n",
    "\n",
    "# create spark session\n",
    "spark = duo_etl.create_spark_session()\n",
    "\n",
    "# read learning traces into df\n",
    "filename = 'learning_traces.csv'\n",
    "lt_df = duo_etl.read_learning_traces(spark, s3_path, filename)\n",
    "\n",
    "# read language reference table into df\n",
    "filename = 'language-codes-full_json.json'\n",
    "lang_df = duo_etl.read_lang_ref(spark, s3_path, filename)\n",
    "\n",
    "# load txt file containing breakdown of lexeme codes\n",
    "filename = 'lexeme_reference.txt'\n",
    "lex_df = duo_etl.read_lex_ref(spark, s3_bucket, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning traces dataset has 2000000 rows\n",
      "language reference dataset has 487 rows\n",
      "lexeme reference dataset has 22 rows\n"
     ]
    }
   ],
   "source": [
    "duo_etl.show_size('learning traces', lt_df)\n",
    "duo_etl.show_size('language reference', lang_df)\n",
    "duo_etl.show_size('lexeme reference', lex_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "An integral part of of data pipelines is performing data quality checks. In this case we are concerned with missing data, and duplicate data.\n",
    "\n",
    "#### Cleaning Steps\n",
    "All of the raw data is checked for missing values, and the entire rows are dropped if found.\n",
    "\n",
    "Duplicate data also presents a problem, so all duplicate rows are dropped from the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean learning traces\n",
    "lt_df = duo_etl.check_data(lt_df, 'learning traces data')\n",
    "\n",
    "# clean language reference\n",
    "lang_df = duo_etl.check_data(lang_df, 'language data', cols=['alpha2', 'English'])\n",
    "\n",
    "# clean lexeme data\n",
    "lex_df = duo_etl.check_data(lex_df, 'lexeme data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "![alt text](schema_diagram.png \"schema\")\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension table: users\n",
    "dim_users = duo_etl.create_users_table(lt_df)\n",
    "\n",
    "# dimension table: times\n",
    "dim_times = duo_etl.create_times_table(lt_df)\n",
    "\n",
    "# dimension table: languages\n",
    "dim_langs = duo_etl.create_langs_table(spark, lt_df, lang_df)\n",
    "\n",
    "# dimension table: words\n",
    "dim_words = duo_etl.create_words_table(lt_df, lex_df)\n",
    "\n",
    "# fact table: word views\n",
    "fact_wordviews = duo_etl.create_wordviews_table(lt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pkey\n",
    "pk = duo_etl.qc_check_pk_unique(dim_users, 'user_id')\n",
    "if pk != True:\n",
    "    print('qc failed for users table')\n",
    "\n",
    "# check pkey\n",
    "pk = duo_etl.qc_check_pk_unique(dim_langs, 'alpha2_code')\n",
    "if pk != True:\n",
    "    print('qc failed for langs table')\n",
    "\n",
    "# check pkey words table\n",
    "pk = duo_etl.qc_check_pk_unique(dim_words, 'lexeme_id')\n",
    "if pk != True:\n",
    "    print('qc failed for words table')\n",
    "\n",
    "# check pkey times table\n",
    "pk = duo_etl.qc_check_pk_unique(dim_times, 'epoch')\n",
    "if pk != True:\n",
    "    print('qc failed for times table')\n",
    "\n",
    "# count number of rows in learning traces... compare with size of word views table\n",
    "count = duo_etl.qc_source_count(lt_df, fact_wordviews)\n",
    "if count != True:\n",
    "    print('qc failed for wordviews table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### write to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# write parquet files to S3\\nduo_etl.upload_parquet(s3_path, output_data, dim_times, 'dim_times.parquet')\\nduo_etl.upload_parquet(s3_path, output_data, dim_langs, 'dim_langs.parquet')\\nduo_etl.upload_parquet(s3_path, output_data, dim_users, 'dim_users.parquet')\\nduo_etl.upload_parquet(s3_path, output_data, dim_words, 'dim_words.parquet')\\nduo_etl.upload_parquet(s3_path, output_data, fact_wordviews, 'fact_wordviews.parquet')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# directory in S3 bucket to store parquet files\n",
    "output_data = 'output_files/'\n",
    "\n",
    "# write parquet files to S3\n",
    "duo_etl.upload_parquet(s3_path, output_data, dim_times, 'dim_times.parquet')\n",
    "duo_etl.upload_parquet(s3_path, output_data, dim_langs, 'dim_langs.parquet')\n",
    "duo_etl.upload_parquet(s3_path, output_data, dim_users, 'dim_users.parquet')\n",
    "duo_etl.upload_parquet(s3_path, output_data, dim_words, 'dim_words.parquet')\n",
    "duo_etl.upload_parquet(s3_path, output_data, fact_wordviews, 'fact_wordviews.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dim_langs\n",
    "##### *alpha2_code*: the two-letter alphanumeric code used by ISO for identifying languages  (from learning traces table)\n",
    "##### *english_name*: the name of the language in English (from language reference table)\n",
    "\n",
    "### dim_times\n",
    "##### *timestamp*: timestamp of session (derived from epoch)\n",
    "##### *epoch*: Unix epoch of session (from learning traces table)\n",
    "##### *hour*: hour of session (derived from epoch)\n",
    "##### *day*: day of session (derived from epoch)\n",
    "##### *week*: week of session (derived from epoch)\n",
    "##### *month*: month of session (derived from epoch)\n",
    "##### *year*: year of session (derived from epoch)\n",
    "##### *weekday*: day of week of session (derived from epoch)\n",
    "\n",
    "### dim_users\n",
    "##### *user_id*: user ID (from learning traces table)\n",
    "##### *number_of_sessions* number of sessions user has logged (derived from learning traces table)\n",
    "\n",
    "### dim_words:\n",
    "##### *lexeme_id*: lexeme ID (from learning traces table)\n",
    "##### *language*: language of the word (from learning traces table)\n",
    "##### *lemma*: lemma of word (derived from learning traces table)\n",
    "##### *surface*: surface of word (derived from learning traces table)\n",
    "##### *part_of_speech*: part of speech of word (derived from learning traces and lexeme reference)\n",
    "\n",
    "### fact_wordviews:\n",
    "##### *timestamp*: timestamp of session\n",
    "##### *user_id*: user ID\n",
    "##### *delta*: time since word last seen\n",
    "##### *learning_language*: language that user is learning\n",
    "##### *ui_language*: language that user is using\n",
    "##### *lexeme_id*: word ID\n",
    "##### *session_pct*: percent that user has gotten the word correct in current session\n",
    "##### *history_pct*: percent that user has gotten the word correct in all previous sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------------+------------------+\n",
      "|learning_language|ui_language| Learning Language|       UI Language|\n",
      "+-----------------+-----------+------------------+------------------+\n",
      "|               pt|         en|        Portuguese|           English|\n",
      "|               de|         en|            German|           English|\n",
      "|               es|         en|Spanish; Castilian|           English|\n",
      "|               it|         en|           Italian|           English|\n",
      "|               fr|         en|            French|           English|\n",
      "|               en|         pt|           English|        Portuguese|\n",
      "|               en|         es|           English|Spanish; Castilian|\n",
      "|               en|         it|           English|           Italian|\n",
      "+-----------------+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view language pairs available\n",
    "lang_pairs = duo_etl.languages_available(fact_wordviews, dim_langs)\n",
    "lang_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+---------------------------+\n",
      "|learning_language|ui_language|number of learners for pair|\n",
      "+-----------------+-----------+---------------------------+\n",
      "|               de|         en|                       4275|\n",
      "|               es|         en|                       9078|\n",
      "|               pt|         en|                        799|\n",
      "|               en|         pt|                       2353|\n",
      "|               en|         es|                       8851|\n",
      "|               en|         it|                        982|\n",
      "|               fr|         en|                       5628|\n",
      "|               it|         en|                       2046|\n",
      "+-----------------+-----------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analysts to see number of users per language pair\n",
    "pair_users = duo_etl.num_users_pair(fact_wordviews)\n",
    "pair_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------------------------+\n",
      "|learning_language|ui_language|number of words seen for pair|\n",
      "+-----------------+-----------+-----------------------------+\n",
      "|               de|         en|                       226055|\n",
      "|               es|         en|                       527346|\n",
      "|               pt|         en|                        46085|\n",
      "|               en|         pt|                       144067|\n",
      "|               en|         es|                       579262|\n",
      "|               en|         it|                        59757|\n",
      "|               fr|         en|                       299490|\n",
      "|               it|         en|                       117938|\n",
      "+-----------------+-----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analysts to see number of words shown per language pair\n",
    "pair_views = duo_etl.num_views_pair(fact_wordviews)\n",
    "pair_views.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses S3 to store the datasets because S3 is a cost effective way to store data in the cloud. Spark is used because it a good tool for wrangling data due to its Python API, and it can scale horizontally so that as the dataset becomes larger, it is able to handle the load.\n",
    "\n",
    "A star schema was used to model the data because each field in the word_views fact table can be further described in a corresponding dimension table. In this use-case using a star schema has the benefit over other schemas because it allows the fact table to be as minimally descriptive as it can be, with more granular information just a JOIN away in a dimension table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be updated any time there is a new learning_traces.csv and an accompanying lexeme_reference.txt dataset released. This is because the learning_traces.csv dataset is what contains the events, and the lexeme_reference.txt pairs with the words in the events. We do not expect the language_reference-json.json dataset to be updated as it is a fixed reference table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data was increased by 100x, a larger Spark cluster would have to be invoked. Likely a managed cluster such as an AWS EMR, or a Databricks cluster. \n",
    "\n",
    "If the data is used to populate a dashboard that must be updated on a daily basis, then Airflow would be used to schedule the loading of the updated learning_traces.csv file, then perform the data modeling, then upload the modeled data to S3 so that the dashboard can fetch the newly updated data.\n",
    "\n",
    "If the database needed to be accessed by 100+ people then a Redshift cluster would be made available for the people who need privilege to the data. This would ensure ACID compliance amongst all users of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
